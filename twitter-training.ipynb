{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re #regular expression\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv('data/training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['target','id','date','flag','user','text']\n",
    "twitter_data = pd.read_csv('data/training.1600000.processed.noemoticon.csv',names = column_names, encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "id        0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.replace({'target':{4:1}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import multiprocessing\n",
    "\n",
    "port_stem = PorterStemmer()\n",
    "\n",
    "def stemming(content, stop_words):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stop_words]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "\n",
    "    return stemmed_content\n",
    "\n",
    "def apply_stemming_parallel(data, stop_words, n_jobs=-1):\n",
    "    with parallel_backend('loky', n_jobs=n_jobs):\n",
    "        return Parallel()(delayed(stemming)(content, stop_words) for content in data)\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "chunk_size = 1000\n",
    "chunks = [twitter_data['text'][i:i+chunk_size] for i in range(0, len(twitter_data['text']), chunk_size)]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmed_contents = Parallel(n_jobs=num_cores)(delayed(apply_stemming_parallel)(chunk, stop_words, n_jobs=-1) for chunk in chunks)\n",
    "\n",
    "stemmed_contents = [item for sublist in stemmed_contents for item in sublist]\n",
    "\n",
    "twitter_data['stemmed_content'] = stemmed_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>switchfoot http twitpic com zl awww bummer sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset updat facebook text might cri result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass behav mad see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     stemmed_content  \n",
       "0  switchfoot http twitpic com zl awww bummer sho...  \n",
       "1  upset updat facebook text might cri result sch...  \n",
       "2  kenichan dive mani time ball manag save rest g...  \n",
       "3                    whole bodi feel itchi like fire  \n",
       "4                      nationwideclass behav mad see  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          switchfoot http twitpic com zl awww bummer sho...\n",
      "1          upset updat facebook text might cri result sch...\n",
      "2          kenichan dive mani time ball manag save rest g...\n",
      "3                            whole bodi feel itchi like fire\n",
      "4                              nationwideclass behav mad see\n",
      "                                 ...                        \n",
      "1599995                           woke school best feel ever\n",
      "1599996    thewdb com cool hear old walt interview http b...\n",
      "1599997                         readi mojo makeov ask detail\n",
      "1599998    happi th birthday boo alll time tupac amaru sh...\n",
      "1599999    happi charitytuesday thenspcc sparkschar speak...\n",
      "Name: stemmed_content, Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(twitter_data['stemmed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "          ..\n",
      "1599995    1\n",
      "1599996    1\n",
      "1599997    1\n",
      "1599998    1\n",
      "1599999    1\n",
      "Name: target, Length: 1600000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(twitter_data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import time\n",
    "from joblib import parallel_backend\n",
    "\n",
    "X = twitter_data['stemmed_content'].values\n",
    "y = twitter_data['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=2**18)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.78871484 0.78938281 0.79153516 0.78776172 0.79020313]\n",
      "Mean CV Accuracy: 0.7895195312500001\n",
      "\n",
      "Test Metrics:\n",
      "Training Time: 22.712084531784058\n",
      "Test Time: 0.10026741027832031\n",
      "Accuracy: 0.791153125\n",
      "Precision: 0.7797380636405671\n",
      "Recall: 0.81155625\n",
      "F1 Score: 0.7953290519371939\n",
      "Area Under ROC Curve: 0.7911531250000001\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = LinearSVC(C=0.1, dual=False, max_iter=1000)\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "with parallel_backend('threading'):\n",
    "    svm_classifier.fit(X_train_vectorized, y_train)\n",
    "svm_train_time = time.time() - start_time\n",
    "\n",
    "# Cross-validation\n",
    "cv_start_time = time.time()\n",
    "cv_scores = cross_val_score(svm_classifier, X_train_vectorized, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "cv_time = time.time() - cv_start_time\n",
    "\n",
    "# Predictions on test set\n",
    "start_time = time.time()\n",
    "svm_test_predictions = svm_classifier.predict(X_test_vectorized)\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "# Evaluation on test set\n",
    "svm_accuracy_test = accuracy_score(y_test, svm_test_predictions)\n",
    "svm_precision_test = precision_score(y_test, svm_test_predictions)\n",
    "svm_recall_test = recall_score(y_test, svm_test_predictions)\n",
    "svm_f1_test = f1_score(y_test, svm_test_predictions)\n",
    "svm_auc_roc_test = roc_auc_score(y_test, svm_test_predictions)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"Training Time:\", svm_train_time)\n",
    "print(\"Test Time:\", test_time)\n",
    "print(\"Accuracy:\", svm_accuracy_test)\n",
    "print(\"Precision:\", svm_precision_test)\n",
    "print(\"Recall:\", svm_recall_test)\n",
    "print(\"F1 Score:\", svm_f1_test)\n",
    "print(\"Area Under ROC Curve:\", svm_auc_roc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.78878906 0.78968359 0.79185938 0.78778516 0.79057813]\n",
      "Mean CV Accuracy: 0.7897390625\n",
      "\n",
      "Test Metrics:\n",
      "Training Time: 116.19415378570557\n",
      "Test Time: 0.07979011535644531\n",
      "Accuracy: 0.79156875\n",
      "Precision: 0.7819132221416485\n",
      "Recall: 0.80869375\n",
      "F1 Score: 0.7950780385891606\n",
      "Area Under ROC Curve: 0.7915687499999999\n"
     ]
    }
   ],
   "source": [
    "# Define and configure the logistic regression classifier\n",
    "logistic_classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "with parallel_backend('threading'):\n",
    "    logistic_classifier.fit(X_train_vectorized, y_train)\n",
    "logistic_train_time = time.time() - start_time\n",
    "\n",
    "# Cross-validation\n",
    "cv_start_time = time.time()\n",
    "cv_scores = cross_val_score(logistic_classifier, X_train_vectorized, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "cv_time = time.time() - cv_start_time\n",
    "\n",
    "# Predictions on test set\n",
    "start_time = time.time()\n",
    "logistic_test_predictions = logistic_classifier.predict(X_test_vectorized)\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "# Evaluation on test set\n",
    "logistic_accuracy_test = accuracy_score(y_test, logistic_test_predictions)\n",
    "logistic_precision_test = precision_score(y_test, logistic_test_predictions)\n",
    "logistic_recall_test = recall_score(y_test, logistic_test_predictions)\n",
    "logistic_f1_test = f1_score(y_test, logistic_test_predictions)\n",
    "logistic_auc_roc_test = roc_auc_score(y_test, logistic_test_predictions)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"Training Time:\", logistic_train_time)\n",
    "print(\"Test Time:\", test_time)\n",
    "print(\"Accuracy:\", logistic_accuracy_test)\n",
    "print(\"Precision:\", logistic_precision_test)\n",
    "print(\"Recall:\", logistic_recall_test)\n",
    "print(\"F1 Score:\", logistic_f1_test)\n",
    "print(\"Area Under ROC Curve:\", logistic_auc_roc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.77157031 0.77291406 0.7763125  0.77202734 0.77311328]\n",
      "Mean CV Accuracy: 0.7731875\n",
      "\n",
      "Test Metrics:\n",
      "Training Time: 0.41886210441589355\n",
      "Test Time: 0.08178091049194336\n",
      "Accuracy: 0.774628125\n",
      "Precision: 0.7748222181915978\n",
      "Recall: 0.774275\n",
      "F1 Score: 0.7745485124434566\n",
      "Area Under ROC Curve: 0.774628125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Define and configure the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "with parallel_backend('threading'):\n",
    "    nb_classifier.fit(X_train_vectorized, y_train)\n",
    "nb_train_time = time.time() - start_time\n",
    "\n",
    "# Cross-validation\n",
    "cv_start_time = time.time()\n",
    "cv_scores = cross_val_score(nb_classifier, X_train_vectorized, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "cv_time = time.time() - cv_start_time\n",
    "\n",
    "\n",
    "# Predictions on test set\n",
    "start_time = time.time()\n",
    "nb_test_predictions = nb_classifier.predict(X_test_vectorized)\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "# Evaluation on test set\n",
    "nb_accuracy_test = accuracy_score(y_test, nb_test_predictions)\n",
    "nb_precision_test = precision_score(y_test, nb_test_predictions)\n",
    "nb_recall_test = recall_score(y_test, nb_test_predictions)\n",
    "nb_f1_test = f1_score(y_test, nb_test_predictions)\n",
    "nb_auc_roc_test = roc_auc_score(y_test, nb_test_predictions)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"Training Time:\", nb_train_time)\n",
    "print(\"Test Time:\", test_time)\n",
    "print(\"Accuracy:\", nb_accuracy_test)\n",
    "print(\"Precision:\", nb_precision_test)\n",
    "print(\"Recall:\", nb_recall_test)\n",
    "print(\"F1 Score:\", nb_f1_test)\n",
    "print(\"Area Under ROC Curve:\", nb_auc_roc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENSEMBLE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.78876953 0.78977734 0.79167969 0.78774219 0.79032813]\n",
      "Mean CV Accuracy: 0.789659375\n",
      "\n",
      "Test Metrics:\n",
      "Training Time: 126.65012764930725\n",
      "Test Time: 1.8575878143310547\n",
      "Accuracy: 0.791178125\n",
      "Precision: 0.7807026528730923\n",
      "Recall: 0.8098375\n",
      "F1 Score: 0.7950032364842271\n",
      "Area Under ROC Curve: 0.791178125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define the ensemble classifier\n",
    "ensemble_classifier = VotingClassifier(estimators=[\n",
    "    ('svm', svm_classifier),\n",
    "    ('logistic', logistic_classifier),\n",
    "    ('nb', nb_classifier)\n",
    "], voting='hard')\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "with parallel_backend('threading', n_jobs=4):\n",
    "    ensemble_classifier.fit(X_train_vectorized, y_train)\n",
    "ensemble_train_time = time.time() - start_time\n",
    "\n",
    "# Cross-validation\n",
    "cv_start_time = time.time()\n",
    "cv_scores = cross_val_score(ensemble_classifier, X_train_vectorized, y_train, cv=5, scoring='accuracy', n_jobs=4)\n",
    "cv_time = time.time() - cv_start_time\n",
    "\n",
    "\n",
    "# Predictions on test set\n",
    "start_time = time.time()\n",
    "ensemble_test_predictions = ensemble_classifier.predict(X_test_vectorized)\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "# Evaluation on test set\n",
    "ensemble_accuracy_test = accuracy_score(y_test, ensemble_test_predictions)\n",
    "ensemble_precision_test = precision_score(y_test, ensemble_test_predictions)\n",
    "ensemble_recall_test = recall_score(y_test, ensemble_test_predictions)\n",
    "ensemble_f1_test = f1_score(y_test, ensemble_test_predictions)\n",
    "ensemble_auc_roc_test = roc_auc_score(y_test, ensemble_test_predictions)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"Training Time:\", ensemble_train_time)\n",
    "print(\"Test Time:\", test_time)\n",
    "print(\"Accuracy:\", ensemble_accuracy_test)\n",
    "print(\"Precision:\", ensemble_precision_test)\n",
    "print(\"Recall:\", ensemble_recall_test)\n",
    "print(\"F1 Score:\", ensemble_f1_test)\n",
    "print(\"Area Under ROC Curve:\", ensemble_auc_roc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.74352734 0.74450781 0.74285156 0.74138672 0.74166406]\n",
      "Mean CV Accuracy: 0.7427874999999999\n",
      "\n",
      "Test Metrics:\n",
      "Training Time: 214.9751410484314\n",
      "Test Time: 18.821688175201416\n",
      "Accuracy: 0.743390625\n",
      "Precision: 0.7293446957873722\n",
      "Recall: 0.7740125\n",
      "F1 Score: 0.7510150121740817\n",
      "Area Under ROC Curve: 0.743390625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define and configure the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=400, max_depth=20, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "with parallel_backend('threading'):\n",
    "    rf_classifier.fit(X_train_vectorized, y_train)\n",
    "rf_train_time = time.time() - start_time\n",
    "\n",
    "# Cross-validation\n",
    "cv_start_time = time.time()\n",
    "cv_scores = cross_val_score(rf_classifier, X_train_vectorized, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "cv_time = time.time() - cv_start_time\n",
    "\n",
    "# Predictions on test set\n",
    "start_time = time.time()\n",
    "rf_test_predictions = rf_classifier.predict(X_test_vectorized)\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "# Evaluation on test set\n",
    "rf_accuracy_test = accuracy_score(y_test, rf_test_predictions)\n",
    "rf_precision_test = precision_score(y_test, rf_test_predictions)\n",
    "rf_recall_test = recall_score(y_test, rf_test_predictions)\n",
    "rf_f1_test = f1_score(y_test, rf_test_predictions)\n",
    "rf_auc_roc_test = roc_auc_score(y_test, rf_test_predictions)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"Training Time:\", rf_train_time)\n",
    "print(\"Test Time:\", test_time)\n",
    "print(\"Accuracy:\", rf_accuracy_test)\n",
    "print(\"Precision:\", rf_precision_test)\n",
    "print(\"Recall:\", rf_recall_test)\n",
    "print(\"F1 Score:\", rf_f1_test)\n",
    "print(\"Area Under ROC Curve:\", rf_auc_roc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate models on the test set\n",
    "models = {\n",
    "    'SVM' : svm_classifier,\n",
    "    'Logistic Regression' : logistic_classifier,\n",
    "    'Naive Bayes' : nb_classifier,\n",
    "    'Random Forest' : rf_classifier,\n",
    "    'Ensemble' : ensemble_classifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "  Training Time: 25.471007823944092 seconds\n",
      "  Accuracy: 79.1153125%\n",
      "  Precision: 0.7916387472714111\n",
      "  Recall: 0.791153125\n",
      "  F1 Score: 0.7910661484472313\n",
      "  AUC: 0.7911531250000001\n",
      "\n",
      "Logistic Regression:\n",
      "  Training Time: 178.20142936706543 seconds\n",
      "  Accuracy: 79.156875%\n",
      "  Precision: 0.7919111800586576\n",
      "  Recall: 0.79156875\n",
      "  F1 Score: 0.7915076063478679\n",
      "  AUC: 0.7915687499999999\n",
      "\n",
      "Naive Bayes:\n",
      "  Training Time: 0.47132253646850586 seconds\n",
      "  Accuracy: 77.4628125%\n",
      "  Precision: 0.7746282619815733\n",
      "  Recall: 0.774628125\n",
      "  F1 Score: 0.7746280968967398\n",
      "  AUC: 0.774628125\n",
      "\n",
      "Random Forest:\n",
      "  Training Time: 237.92444109916687 seconds\n",
      "  Accuracy: 74.3390625%\n",
      "  Precision: 0.7443069708322817\n",
      "  Recall: 0.743390625\n",
      "  F1 Score: 0.7431497767438087\n",
      "  AUC: 0.743390625\n",
      "\n",
      "Ensemble:\n",
      "  Training Time: 158.98287153244019 seconds\n",
      "  Accuracy: 79.1178125%\n",
      "  Precision: 0.7915842111530605\n",
      "  Recall: 0.791178125\n",
      "  F1 Score: 0.7911053936896042\n",
      "  AUC: 0.791178125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "    auc = roc_auc_score(y_test, predictions, average='weighted', multi_class='ovr')\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Training Time: {training_time} seconds\")\n",
    "    print(f\"  Accuracy: {accuracy*100}%\")\n",
    "    print(f\"  Precision: {precision}\")\n",
    "    print(f\"  Recall: {recall}\")\n",
    "    print(f\"  F1 Score: {f1}\")\n",
    "    print(f\"  AUC: {auc}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "  Training Time: 23.858806848526 seconds\n",
      "  Accuracy: 79.1153125%\n",
      "  Precision: 0.7916387472714111\n",
      "  Recall: 0.791153125\n",
      "  F1 Score: 0.7910661484472313\n",
      "  AUC: 0.7911531250000001\n",
      "\n",
      "Logistic Regression:\n",
      "  Training Time: 147.02249765396118 seconds\n",
      "  Accuracy: 79.156875%\n",
      "  Precision: 0.7919111800586576\n",
      "  Recall: 0.79156875\n",
      "  F1 Score: 0.7915076063478679\n",
      "  AUC: 0.7915687499999999\n",
      "\n",
      "Naive Bayes:\n",
      "  Training Time: 0.5826525688171387 seconds\n",
      "  Accuracy: 77.4628125%\n",
      "  Precision: 0.7746282619815733\n",
      "  Recall: 0.774628125\n",
      "  F1 Score: 0.7746280968967398\n",
      "  AUC: 0.774628125\n",
      "\n",
      "Random Forest:\n",
      "  Training Time: 253.37848567962646 seconds\n",
      "  Accuracy: 74.3390625%\n",
      "  Precision: 0.7443069708322817\n",
      "  Recall: 0.743390625\n",
      "  F1 Score: 0.7431497767438087\n",
      "  AUC: 0.743390625\n",
      "\n",
      "Ensemble:\n",
      "  Training Time: 203.08208203315735 seconds\n",
      "  Accuracy: 79.1178125%\n",
      "  Precision: 0.7915842111530605\n",
      "  Recall: 0.791178125\n",
      "  F1 Score: 0.7911053936896042\n",
      "  AUC: 0.791178125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store performance metrics\n",
    "performance_metrics = {\n",
    "    'Model': [],\n",
    "    'Training Time': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'AUC': []\n",
    "}\n",
    "\n",
    "# Iterate over each model\n",
    "models = {\n",
    "    'SVM' : svm_classifier,\n",
    "    'Logistic Regression' : logistic_classifier,\n",
    "    'Naive Bayes' : nb_classifier,\n",
    "    'Random Forest' : rf_classifier,\n",
    "    'Ensemble' : ensemble_classifier\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "    auc = roc_auc_score(y_test, predictions, average='weighted', multi_class='ovr')\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Training Time: {training_time} seconds\")\n",
    "    print(f\"  Accuracy: {accuracy*100}%\")\n",
    "    print(f\"  Precision: {precision}\")\n",
    "    print(f\"  Recall: {recall}\")\n",
    "    print(f\"  F1 Score: {f1}\")\n",
    "    print(f\"  AUC: {auc}\")\n",
    "    print()\n",
    "    \n",
    "    # Add metrics to the dictionary\n",
    "    performance_metrics['Model'].append(name)\n",
    "    performance_metrics['Training Time'].append(training_time)\n",
    "    performance_metrics['Accuracy'].append(accuracy*100)\n",
    "    performance_metrics['Precision'].append(precision)\n",
    "    performance_metrics['Recall'].append(recall)\n",
    "    performance_metrics['F1 Score'].append(f1)\n",
    "    performance_metrics['AUC'].append(auc)\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "metrics_df = pd.DataFrame(performance_metrics)\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "metrics_df.to_csv('twitter_performance_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store confusion matrices\n",
    "confusion_matrices = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    # Store confusion matrix in the dictionary\n",
    "    confusion_matrices[name] = cm\n",
    "    \n",
    "    # Save confusion matrix to a pickle file\n",
    "    filename = f'twitter_{name}_cm.pkl'\n",
    "    joblib.dump(cm, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_roc_curve_data.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a dictionary to store ROC curve data\n",
    "roc_curve_data = {}\n",
    "\n",
    "# Iterate over each model\n",
    "for name, model in models.items():\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "    \n",
    "    # Compute false positive rate, true positive rate, and thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "    \n",
    "    # Store ROC curve data in the dictionary\n",
    "    roc_curve_data[name] = {'fpr': fpr, 'tpr': tpr}\n",
    "\n",
    "# Save ROC curve data to a Joblib file\n",
    "joblib.dump(roc_curve_data, 'twitter_roc_curve_data.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
